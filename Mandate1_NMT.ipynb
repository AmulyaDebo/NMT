{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e13eb076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "108/108 [==============================] - 91s 825ms/step - loss: 2.9967 - accuracy: 0.4386 - val_loss: 2.4427 - val_accuracy: 0.4877\n",
      "Epoch 2/20\n",
      "108/108 [==============================] - 93s 866ms/step - loss: 2.3153 - accuracy: 0.4999 - val_loss: 2.2282 - val_accuracy: 0.5065\n",
      "Epoch 3/20\n",
      "108/108 [==============================] - 93s 859ms/step - loss: 2.1253 - accuracy: 0.5102 - val_loss: 1.9606 - val_accuracy: 0.5166\n",
      "Epoch 4/20\n",
      "108/108 [==============================] - 91s 841ms/step - loss: 1.8480 - accuracy: 0.5461 - val_loss: 1.7557 - val_accuracy: 0.5634\n",
      "Epoch 5/20\n",
      "108/108 [==============================] - 91s 845ms/step - loss: 1.7004 - accuracy: 0.5690 - val_loss: 1.6456 - val_accuracy: 0.5756\n",
      "Epoch 6/20\n",
      "108/108 [==============================] - 93s 859ms/step - loss: 1.6081 - accuracy: 0.5814 - val_loss: 1.5678 - val_accuracy: 0.5882\n",
      "Epoch 7/20\n",
      "108/108 [==============================] - 93s 858ms/step - loss: 1.5418 - accuracy: 0.5899 - val_loss: 1.5112 - val_accuracy: 0.5988\n",
      "Epoch 8/20\n",
      "108/108 [==============================] - 92s 848ms/step - loss: 1.4848 - accuracy: 0.5995 - val_loss: 1.4558 - val_accuracy: 0.6058\n",
      "Epoch 9/20\n",
      "108/108 [==============================] - 92s 851ms/step - loss: 1.4372 - accuracy: 0.6090 - val_loss: 1.4140 - val_accuracy: 0.6145\n",
      "Epoch 10/20\n",
      "108/108 [==============================] - 90s 835ms/step - loss: 1.4149 - accuracy: 0.6135 - val_loss: 1.3894 - val_accuracy: 0.6196\n",
      "Epoch 11/20\n",
      "108/108 [==============================] - 87s 804ms/step - loss: 1.3705 - accuracy: 0.6218 - val_loss: 1.3529 - val_accuracy: 0.6279\n",
      "Epoch 12/20\n",
      "108/108 [==============================] - 89s 820ms/step - loss: 1.3437 - accuracy: 0.6273 - val_loss: 1.3372 - val_accuracy: 0.6273\n",
      "Epoch 13/20\n",
      "108/108 [==============================] - 87s 805ms/step - loss: 1.3266 - accuracy: 0.6290 - val_loss: 1.3158 - val_accuracy: 0.6302\n",
      "Epoch 14/20\n",
      "108/108 [==============================] - 87s 807ms/step - loss: 1.3090 - accuracy: 0.6309 - val_loss: 1.2987 - val_accuracy: 0.6321\n",
      "Epoch 15/20\n",
      "108/108 [==============================] - 87s 806ms/step - loss: 1.2940 - accuracy: 0.6325 - val_loss: 1.2827 - val_accuracy: 0.6344\n",
      "Epoch 16/20\n",
      "108/108 [==============================] - 87s 806ms/step - loss: 1.2766 - accuracy: 0.6349 - val_loss: 1.2670 - val_accuracy: 0.6360\n",
      "Epoch 17/20\n",
      "108/108 [==============================] - 87s 805ms/step - loss: 1.2605 - accuracy: 0.6367 - val_loss: 1.2491 - val_accuracy: 0.6381\n",
      "Epoch 18/20\n",
      "108/108 [==============================] - 94s 872ms/step - loss: 1.2502 - accuracy: 0.6375 - val_loss: 1.2374 - val_accuracy: 0.6398\n",
      "Epoch 19/20\n",
      "108/108 [==============================] - 91s 841ms/step - loss: 1.2333 - accuracy: 0.6404 - val_loss: 1.2290 - val_accuracy: 0.6407\n",
      "Epoch 20/20\n",
      "108/108 [==============================] - 87s 807ms/step - loss: 1.2219 - accuracy: 0.6415 - val_loss: 1.2138 - val_accuracy: 0.6433\n",
      "new jersey est parfois froid en mois et il est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
    "# Import the necessary libraries\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# load the data set\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load dataset\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data.split('\\n')\n",
    "\n",
    "# Updates internal vocabulary based on a list of texts.\n",
    "\n",
    "def tokenize(x):\n",
    "    x_tk = Tokenizer(char_level = False)\n",
    "    x_tk.fit_on_texts(x)\n",
    "    return x_tk.texts_to_sequences(x), x_tk\n",
    "\n",
    "english_sentences = load_data('smallvocab_en.txt') # Download the input vocabulary\n",
    "french_sentences = load_data('smallvocab_fr.txt')  # Download the output vocabulary\n",
    "\n",
    "# Pad each sentence\n",
    "\n",
    "def pad(x, length=None):\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "    return pad_sequences(x, maxlen = length, padding = 'post') # Pads sequences to the same length.\n",
    "\n",
    "# Preproceesing the data\n",
    "\n",
    "def preprocess(x, y):\n",
    "    \n",
    "    # First tokenize\n",
    "    \n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "    \n",
    "    # Pad the vectors\n",
    "    \n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "    \n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "   \n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "# Process the input and output vocabulary\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "\n",
    "\n",
    "\n",
    "def logits_to_text(logits, tokenizer):\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "# The encode decode model\n",
    "\n",
    "def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "  \n",
    "    learning_rate = 1e-3\n",
    "    model = Sequential()\n",
    "    # LSTM model takes a lot of time. So GRU is used\n",
    "    model.add(GRU(128, input_shape = input_shape[1:], return_sequences = False))\n",
    "    model.add(RepeatVector(output_sequence_length)) # Repeats the vector\n",
    "    model.add(GRU(128, return_sequences = True))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation = 'softmax'))) # Applies a layer\n",
    "    \n",
    "    model.compile(loss = sparse_categorical_crossentropy, \n",
    "                 optimizer = Adam(learning_rate), \n",
    "                 metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# This is for testing the encode-decode model\n",
    "\n",
    "def test_encdec_model(encdec_model):\n",
    "    input_shape = (137861, 15, 1)\n",
    "    output_sequence_length = 21\n",
    "    english_vocab_size = 199\n",
    "    french_vocab_size = 344\n",
    "    model = encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n",
    "\n",
    "\n",
    "tmp_x = pad(preproc_english_sentences) # Full complete preprocessed input\n",
    "tmp_x = tmp_x.reshape((-1, preproc_english_sentences.shape[1], 1)) # We need to reshape it for the above functions to work well\n",
    "\n",
    "encodeco_model = encdec_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)\n",
    "\n",
    "encodeco_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=20, validation_split=0.2) # Trains the model\n",
    "print(logits_to_text(encodeco_model.predict(tmp_x[:2])[0], french_tokenizer)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d97cc3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated(src=it, dest=en, text=The capital of India is Delhi, pronunciation=None, extra_data=\"{'confiden...\")\n"
     ]
    }
   ],
   "source": [
    "import googletrans\n",
    "from langdetect import detect\n",
    "from googletrans import Translator  \n",
    "\n",
    "#simple function to detect and translate text \n",
    "def detect_and_translate(text,target_lang):\n",
    "    \n",
    "    result_lang = detect(text)\n",
    "    \n",
    "    if result_lang == target_lang:\n",
    "        return text \n",
    "    \n",
    "    else:\n",
    "        translator = Translator()\n",
    "        translate_text = translator.translate(text,dest = target_lang)\n",
    "        return translate_text \n",
    "\n",
    "sentence = \"La capitale dell'India Ã¨ Delhi\"\n",
    "\n",
    "print(detect_and_translate(sentence,target_lang='en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91665588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
